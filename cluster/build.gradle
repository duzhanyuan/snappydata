/*
 * Copyright (c) 2016 SnappyData, Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */

apply plugin: 'scala'

compileScala.options.encoding = 'UTF-8'

// fix scala+java mix to all use compileScala which uses correct dependency order
sourceSets.main.scala.srcDir 'src/main/java'
sourceSets.test.scala.srcDirs = [ 'src/test/java', 'src/test/scala', 'src/dunit/scala' ]
sourceSets.main.java.srcDirs = []
sourceSets.test.java.srcDirs = [ ]

dependencies {
  compile 'org.scala-lang:scala-library:' + scalaVersion
  compile 'org.scala-lang:scala-reflect:' + scalaVersion
  compile 'org.scala-lang:scala-compiler:' + scalaVersion

  compile 'org.slf4j:slf4j-api:' + slf4jVersion
  compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion
  compile 'org.slf4j:jcl-over-slf4j:' + slf4jVersion
  compile 'org.slf4j:jul-to-slf4j:' + slf4jVersion

  if (new File(rootDir, 'spark/build.gradle').exists()) {
    compile project(':snappy-spark:snappy-spark-core_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-catalyst_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-sql_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-hive_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-repl_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-streaming_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-streaming-kafka-0.8_' + scalaBinaryVersion)
    // compile project(':snappy-spark:snappy-spark-streaming-kafka-0.10_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-sql-kafka-0.10_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-mllib_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-yarn_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-graphx_' + scalaBinaryVersion)
    compile project(':snappy-spark:snappy-spark-hive-thriftserver_' + scalaBinaryVersion)

    testCompile project(path: ':snappy-spark:snappy-spark-sql_' + scalaBinaryVersion,
        configuration: 'testOutput')
    testCompile group: 'org.apache.kafka', name: 'kafka_' + scalaBinaryVersion, version: '0.10.0.1'
  } else {
    compile 'io.snappydata:snappy-spark-core_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-catalyst_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-sql_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-hive_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-repl_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-streaming_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-streaming-kafka-0.8_' + scalaBinaryVersion + ':' + snappySparkVersion
    // compile 'io.snappydata:snappy-spark-streaming-kafka-0.10_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-sql-kafka-0.10_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-mllib_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-yarn_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-graphx_' + scalaBinaryVersion + ':' + snappySparkVersion
    compile 'io.snappydata:snappy-spark-hive-thriftserver_' + scalaBinaryVersion + ':' + snappySparkVersion

    testCompile group: 'io.snappydata', name: 'snappy-spark-sql_' + scalaBinaryVersion,
                version: snappySparkVersion, classifier: 'tests'
    testCompile group: 'org.apache.kafka', name: 'kafka_' + scalaBinaryVersion, version: '0.10.0.1'
  }
  compile (project(':snappy-core_' + scalaBinaryVersion)) {
    exclude(group: 'org.apache.spark', module: 'spark-core_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-catalyst_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-sql_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-hive_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-streaming_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-streaming-kafka-0-8_' + scalaBinaryVersion)
    // exclude(group: 'org.apache.spark', module: 'spark-streaming-kafka-0-10_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-sql-kafka-0-10_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-mllib_' + scalaBinaryVersion)
    exclude(group: 'org.eclipse.jetty', module: 'jetty-servlet')
  }

  if (new File(rootDir, 'store/build.gradle').exists()) {
    testCompile project(path: ':snappy-store:gemfirexd-tools', configuration: 'testOutput')
  } else {
    testCompile group: 'io.snappydata', name: 'snappydata-store-tools', version: snappyStoreVersion, classifier: 'tests'
  }
  if (new File(rootDir, 'spark-jobserver/build.gradle').exists()) {
    compile project(':spark-jobserver_' + scalaBinaryVersion)
  } else {
    compile group: 'io.snappydata', name: 'spark-jobserver_' + scalaBinaryVersion, version: '0.6.3-2'
  }

  testCompile project(':dunit')
  testCompile (project(path: ':snappy-core_' + scalaBinaryVersion, configuration: 'testOutput')) {
    exclude(group: 'org.apache.spark', module: 'spark-core_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-catalyst_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-sql_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-hive_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-streaming_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-streaming-kafka-0-8_' + scalaBinaryVersion)
    // exclude(group: 'org.apache.spark', module: 'spark-streaming-kafka-0-10_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-sql-kafka-0-10_' + scalaBinaryVersion)
    exclude(group: 'org.apache.spark', module: 'spark-mllib_' + scalaBinaryVersion)
    exclude(group: 'org.eclipse.jetty', module: 'jetty-servlet')
  }
  testCompile "org.scalatest:scalatest_${scalaBinaryVersion}:${scalatestVersion}"

  testRuntime "org.pegdown:pegdown:${pegdownVersion}"
}

// Creates the version properties file and writes it to the resources dir
task createVersionPropertiesFile(dependsOn: 'processResources') {
  def propertiesDir = file("${buildDir}/resources/main/io/snappydata")
  outputs.file "${propertiesDir}/SnappyDataVersion.properties"
  inputs.dir compileJava.destinationDir

  doLast {

    def props = [
      'Product-Name'      : productName,
      'Product-Version'   : version,
      'Build-Id'          : System.env.USER + ' ' + buildNumber,
      'Build-Date'        : buildDate,
      'Build-Platform'    : osName.getName() + osVersion + osArch,
      'Build-Java-Version': jdkVersion,
      'Source-Date'       : sourceDate,
      'Source-Revision'   : commitId,
      'Source-Repository' : gitBranch,
    ]

   writeProperties(propertiesDir, 'SnappyDataVersion.properties',
        "Properties that control what version ${productName} will think it is. Changing these values may cause ${productName} to no longer function.", props)
  }
}

compileJava.dependsOn createVersionPropertiesFile


task packageScalaDocs(type: Jar, dependsOn: scaladoc) {
  classifier = 'javadoc'
  from scaladoc
}
if (rootProject.hasProperty('enablePublish')) {
  artifacts {
    archives packageScalaDocs, packageSources
  }
}

testClasses.doLast {
  copyTestsCommonResources(buildDir)
}


def copyDirs(def srcDir, def destDir) {
  mkdir(destDir)
  copy {
    from srcDir
    into destDir
  }
}

test.dependsOn ':cleanJUnit'
scalaTest {
  dependsOn ':cleanScalaTest'
  doFirst {
    // cleanup files since scalatest plugin does not honour workingDir yet
    cleanIntermediateFiles(project.path)
  }
  doLast {
    // cleanup files since scalatest plugin does not honour workingDir yet
    cleanIntermediateFiles(project.path)
  }
}
check.dependsOn test, scalaTest, dunitTest
archivesBaseName = 'snappydata-cluster_' + scalaBinaryVersion

